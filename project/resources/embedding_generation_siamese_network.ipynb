{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Siamese Networks for Embedding Generation\n",
        "\n",
        "This notebook demonstrates how to build and train a Siamese Network to generate vector embeddings. Siamese Networks consist of two identical subnetworks that merge at their outputs, designed to learn embeddings so that similar items are closer in the embedding space compared to dissimilar items. This approach is particularly useful for tasks such as face recognition, signature verification, and in general, for learning a similarity metric between two inputs.\n",
        "\n",
        "## What are Siamese Networks?\n",
        "\n",
        "Siamese Networks are a type of neural network architecture that contain two or more identical subnetworks meant to process two separate inputs. The idea is that by comparing these inputs as they pass through the network, the model learns to distinguish between them based on their similarity. This training method allows the network to effectively generate embeddings â€“ representations of input data in a lower-dimensional space.\n",
        "\n",
        "## Why Generate Embeddings?\n",
        "\n",
        "Generating embeddings can significantly enhance performance on tasks like classification, recommendation, and clustering by representing complex data (like images or text) in a form that highlights their relative similarities or differences. Embeddings make it possible to perform these tasks even with limited data, as they capture the essence of the data's features in a compressed form.\n",
        "\n",
        "## How to Implement a Siamese Network\n",
        "\n",
        "This notebook covers the following steps:\n",
        "\n",
        "1. **Environment Setup**: Installation of necessary libraries (TensorFlow).\n",
        "2. **Siamese Network Architecture**: Designing the architecture using TensorFlow/Keras.\n",
        "3. **Data Preparation**: Preparing pairs of data for training.\n",
        "4. **Model Training**: Training the Siamese Network to generate embeddings.\n",
        "5. **Embedding Generation**: Using the trained network to generate vector embeddings for new data.\n",
        "\n",
        "For more detailed information on Siamese Networks and embedding generation, the following resources might be helpful:\n",
        "\n",
        "- Siamese Networks: [Understanding Siamese Networks](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "- Vector Embeddings: [Learning Embeddings](https://towardsdatascience.com/understanding-feature-vectors-extraction-cdff276b9b65)\n",
        "- Embedding Generation: [Generating Embeddings with Neural Networks](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n"
      ],
      "metadata": {
        "id": "jvWzrw_md7Sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me_FnhGCYhuZ",
        "outputId": "a75e7eaf-5478-4409-c145-95fb5f1070a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas numpy tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import random"
      ],
      "metadata": {
        "id": "bpTVUyB7YqCG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pairs(dataset, num_classes):\n",
        "    (train_digits, train_labels), (test_digits, test_labels) = dataset.load_data()\n",
        "    # Normalize image vectors\n",
        "    train_digits = train_digits.astype('float32') / 255.0\n",
        "    test_digits = test_digits.astype('float32') / 255.0\n",
        "\n",
        "    # Expand dimensions to add channel information\n",
        "    train_digits = np.expand_dims(train_digits, axis=-1)\n",
        "    test_digits = np.expand_dims(test_digits, axis=-1)\n",
        "\n",
        "    # Create pairs\n",
        "    digit_indices = [np.where(train_labels == i)[0] for i in range(num_classes)]\n",
        "    tr_pairs, tr_y = create_pairs(train_digits, digit_indices)\n",
        "\n",
        "    digit_indices = [np.where(test_labels == i)[0] for i in range(num_classes)]\n",
        "    te_pairs, te_y = create_pairs(test_digits, digit_indices)\n",
        "\n",
        "    return (tr_pairs, tr_y), (te_pairs, te_y)\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    \"\"\"Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
        "\n",
        "    for d in range(10):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, 10)\n",
        "            dn = (d + inc) % 10\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "num_classes = 10\n",
        "(tr_pairs, tr_y), (te_pairs, te_y) = load_pairs(mnist, num_classes)"
      ],
      "metadata": {
        "id": "FCRmbTFPYxSM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_base_network(input_shape):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "# Network definition\n",
        "input_shape = tr_pairs[:, 0].shape[1:]\n",
        "base_network = initialize_base_network(input_shape)\n",
        "\n",
        "input_a = Input(shape=input_shape)\n",
        "input_b = Input(shape=input_shape)\n",
        "\n",
        "# Because we re-use the same instance `base_network`,\n",
        "# the weights of the network will be shared across the two branches\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "model = Model([input_a, input_b], distance)"
      ],
      "metadata": {
        "id": "1oSYcVhPY1sF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftB9y3eSZHYw",
        "outputId": "7725e0a8-68e6-4bec-b672-8589429af751"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "847/847 [==============================] - 162s 190ms/step - loss: 0.7308 - accuracy: 0.4703 - val_loss: 0.7339 - val_accuracy: 0.4642\n",
            "Epoch 2/10\n",
            "847/847 [==============================] - 152s 179ms/step - loss: 0.7146 - accuracy: 0.4899 - val_loss: 0.7278 - val_accuracy: 0.4681\n",
            "Epoch 3/10\n",
            "847/847 [==============================] - 156s 184ms/step - loss: 0.7056 - accuracy: 0.5084 - val_loss: 0.7246 - val_accuracy: 0.4668\n",
            "Epoch 4/10\n",
            "847/847 [==============================] - 150s 177ms/step - loss: 0.6989 - accuracy: 0.5200 - val_loss: 0.7239 - val_accuracy: 0.4612\n",
            "Epoch 5/10\n",
            "847/847 [==============================] - 150s 177ms/step - loss: 0.6936 - accuracy: 0.5344 - val_loss: 0.7228 - val_accuracy: 0.4643\n",
            "Epoch 6/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6896 - accuracy: 0.5436 - val_loss: 0.7234 - val_accuracy: 0.4614\n",
            "Epoch 7/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6872 - accuracy: 0.5485 - val_loss: 0.7248 - val_accuracy: 0.4694\n",
            "Epoch 8/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6828 - accuracy: 0.5596 - val_loss: 0.7222 - val_accuracy: 0.4701\n",
            "Epoch 9/10\n",
            "847/847 [==============================] - 162s 192ms/step - loss: 0.6806 - accuracy: 0.5639 - val_loss: 0.7210 - val_accuracy: 0.4727\n",
            "Epoch 10/10\n",
            "847/847 [==============================] - 150s 178ms/step - loss: 0.6784 - accuracy: 0.5693 - val_loss: 0.7234 - val_accuracy: 0.4702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just to get something to test the newly trained network on quickly we reload data from MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Select an image for testing\n",
        "# For simplicity, we're taking the first image from the test set\n",
        "new_image_array = test_images[0]\n",
        "\n",
        "# Siamese networks expect input data to have a specific shape, often including the channel dimension.\n",
        "# MNIST images are 28x28 and grayscale, so we need to reshape them to 28x28x1.\n",
        "new_image_array = np.expand_dims(new_image_array, axis=-1)\n",
        "\n",
        "# Normalize the image\n",
        "new_image_array = new_image_array.astype('float32') / 255.0\n",
        "\n",
        "new_image_embedding = base_network.predict(np.array([new_image_array]))\n",
        "\n",
        "print(\"Generated Embedding:\", new_image_embedding)\n",
        "new_image_embedding.shape # A vector of length 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teFNVfYcZLSM",
        "outputId": "f629c5ca-14b2-4899-abe7-65980625562b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Generated Embedding: [[0.         0.258149   0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.27202314\n",
            "  0.33327174 0.18082604 0.         0.         0.         0.24079165\n",
            "  0.         0.         0.         0.37266743 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.29302543\n",
            "  0.286541   0.         0.         0.         0.         0.\n",
            "  0.24154699 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.28360844 0.38187942 0.         0.         0.\n",
            "  0.         0.         0.39938608 0.         0.42577994 0.\n",
            "  0.         0.         0.31317064 0.41224316 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.40891406\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.42736298 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.28238726 0.         0.37378857 0.         0.\n",
            "  0.         0.         0.         0.3613243  0.         0.\n",
            "  0.         0.         0.         0.         0.49152645 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.27323365 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('siamese_network.h5')  # saves the entire model to a HDF5 file\n",
        "model.save_weights('siamese_network_weights.h5')  # saves just the model weights\n",
        "base_network.save('base_network.h5') # saves just base network, for actually generating embedding rather than the full siamese architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u1t8ngvZ38c",
        "outputId": "80abf6b2-eaec-459d-a70e-d046ca7b90a3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('siamese_network.h5') # load the entire model\n",
        "model.load_weights('siamese_network_weights.h5') # If you've only saved the weights and want to load them, you'll need to define the model architecture first and then load the weights\n",
        "loaded_base_network = load_model('base_network.h5') # loading the base network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2NmBBZCiU7u",
        "outputId": "c5207ba3-c9c2-4286-8e30-b474e41c9065"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfL43rFdjdC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}