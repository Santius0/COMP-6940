{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Lab Introduction\n",
        "\n",
        "Welcome to our PySpark Lab! In this session, we'll be diving into the world of big data processing with Apache Spark and its Python API, PySpark.\n",
        "\n",
        "## What is Apache Spark?\n",
        "\n",
        "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general computation graphs for data analysis. Spark is designed for both batch and real-time data processing, making it a versatile tool for a wide range of data processing tasks.\n",
        "\n",
        "## What is PySpark?\n",
        "\n",
        "PySpark is the Python API for Spark, allowing data scientists and analysts familiar with Python to leverage Spark's powerful data processing capabilities. PySpark provides a way to scale up your data processing tasks from a single machine to a cluster, enabling analysis of large datasets that wouldn't fit into the memory of a single machine.\n",
        "\n",
        "## Using Google Colab for PySpark\n",
        "\n",
        "For this lab, we'll be using Google Colab, a cloud-based Python notebook environment that provides free access to computing resources, including CPUs and GPUs. Google Colab allows us to run PySpark without any setup on our local machines, making it an excellent platform for learning and experimentation.\n",
        "\n",
        "### Setting Up PySpark in Google Colab\n",
        "\n",
        "To get started with PySpark in Google Colab, we'll first need to install PySpark. Don't worry, we'll guide you through this process in the lab. Here's a sneak peek of the commands you'll run:\n",
        "\n",
        "```python\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "```\n",
        "\n",
        "## Running PySpark Code\n",
        "\n",
        "Once PySpark is installed, you can start a Spark session and begin processing data.\n",
        "\n",
        "### Setting Up Spark on Your Own Machine\n",
        "\n",
        "While Google Colab provides a convenient platform for working with PySpark, you might want to set up Spark on your own machine for more control over your development environment or for projects that require specific configurations. Installing and configuring Spark on your own machine can be a valuable learning experience and can give you more flexibility for developing large-scale data processing applications."
      ],
      "metadata": {
        "id": "ik_nzqayTK9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQM-pjHPNns-",
        "outputId": "690c833c-4cef-41f6-a61a-0333d23a2d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Basics\n",
        "### Creating RDDs\n",
        "First import pyspark then create a SparkContext"
      ],
      "metadata": {
        "id": "WB7NIBZbUQ1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "oFV1hOD9N2VH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "QBteMIUNOKts"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [num for num in range(1,10)]\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYZ_NKgqN8ul",
        "outputId": "63ae5800-d288-4203-adf6-4805607261fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "bJp7vVjkOPUl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(myRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p4PDQXOUq2Q",
        "outputId": "7f799cfd-1450-40ae-d962-16a228130fbf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(myRDD.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ussQvLZUtBV",
        "outputId": "50090c35-64a9-4f99-e8c3-5bbbb89e022a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2 creating RDDs from key value pairs (tuples)"
      ],
      "metadata": {
        "id": "dFBemxN9Uyco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kv = [('a',7), ('a', 2), ('b', 2), ('b',4), ('c',1), ('c',2), ('c',3), ('c',4)]\n",
        "print(kv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyPrkcThUvGJ",
        "outputId": "d95d8444-f50a-4809-b02d-b12873a6ece5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = sc.parallelize(kv)\n",
        "print(rdd2.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULUVT9xqUzfF",
        "outputId": "48bc1002-17e3-48d6-9884-736f3e6156ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = rdd2.reduceByKey(lambda x, y: x+y)\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2lSc3KdU5Zt",
        "outputId": "c429dc94-b465-46ad-a5b8-a750ec60c7e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', 6), ('c', 10), ('a', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4 = rdd2.groupByKey()\n",
        "print(rdd4.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhfUrH61U7se",
        "outputId": "464918d3-cb02-4a5d-c2b4-748f6cacfc98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', <pyspark.resultiterable.ResultIterable object at 0x7ca81e28e440>), ('c', <pyspark.resultiterable.ResultIterable object at 0x7ca81e28e290>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7ca81e28ee60>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4.map(lambda x: (x[0], list(x[1]))).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA5maZwOU9gj",
        "outputId": "49f15fcb-b7ce-44bb-87e2-c4bee2a53c9e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [2, 4]), ('c', [1, 2, 3, 4]), ('a', [7, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OT8b-_nQU_o3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}